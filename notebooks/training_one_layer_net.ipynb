{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is for training one-layer net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is defines inside /models/one_layer_net.py. The training loop is inside train_model.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cifar10_classifier.models.one_layer_net import OneLayerNet\n",
    "from cifar10_classifier.scripts import train_model\n",
    "from cifar10_classifier.scripts.data_processing import create_dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"{device} is available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dataloader = create_dataloaders('cifar10_classifier/data/raw/train_dev', 'cifar10_classifier/data/raw/trainLabelsDev.csv', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------\n",
      "loss: 2.320657 batch:     0\n",
      "loss: 1.863511 batch:     8\n",
      "loss: 1.955188 batch:    16\n",
      "Epoch 2\n",
      "-------------------\n",
      "loss: 1.557942 batch:     0\n",
      "loss: 1.444277 batch:     8\n",
      "loss: 1.622667 batch:    16\n",
      "Epoch 3\n",
      "-------------------\n",
      "loss: 1.384988 batch:     0\n",
      "loss: 1.444230 batch:     8\n",
      "loss: 1.188576 batch:    16\n",
      "Epoch 4\n",
      "-------------------\n",
      "loss: 1.062764 batch:     0\n",
      "loss: 1.139835 batch:     8\n",
      "loss: 1.272102 batch:    16\n",
      "Epoch 5\n",
      "-------------------\n",
      "loss: 1.310549 batch:     0\n",
      "loss: 1.056641 batch:     8\n",
      "loss: 1.039118 batch:    16\n",
      "Epoch 6\n",
      "-------------------\n",
      "loss: 0.833290 batch:     0\n",
      "loss: 0.705843 batch:     8\n",
      "loss: 0.996518 batch:    16\n",
      "Epoch 7\n",
      "-------------------\n",
      "loss: 0.946780 batch:     0\n",
      "loss: 0.705251 batch:     8\n",
      "loss: 0.757210 batch:    16\n",
      "Epoch 8\n",
      "-------------------\n",
      "loss: 0.652836 batch:     0\n",
      "loss: 0.898015 batch:     8\n",
      "loss: 0.825711 batch:    16\n",
      "Epoch 9\n",
      "-------------------\n",
      "loss: 0.655947 batch:     0\n",
      "loss: 0.618351 batch:     8\n",
      "loss: 0.627416 batch:    16\n",
      "Epoch 10\n",
      "-------------------\n",
      "loss: 0.498543 batch:     0\n",
      "loss: 0.506440 batch:     8\n",
      "loss: 0.547863 batch:    16\n",
      "Epoch 11\n",
      "-------------------\n",
      "loss: 0.366724 batch:     0\n",
      "loss: 0.465727 batch:     8\n",
      "loss: 0.459639 batch:    16\n",
      "Epoch 12\n",
      "-------------------\n",
      "loss: 0.340743 batch:     0\n",
      "loss: 0.404306 batch:     8\n",
      "loss: 0.342820 batch:    16\n",
      "Epoch 13\n",
      "-------------------\n",
      "loss: 0.291264 batch:     0\n",
      "loss: 0.326072 batch:     8\n",
      "loss: 0.308413 batch:    16\n",
      "Epoch 14\n",
      "-------------------\n",
      "loss: 0.246247 batch:     0\n",
      "loss: 0.313068 batch:     8\n",
      "loss: 0.273574 batch:    16\n",
      "Epoch 15\n",
      "-------------------\n",
      "loss: 0.226661 batch:     0\n",
      "loss: 0.306273 batch:     8\n",
      "loss: 0.197292 batch:    16\n",
      "Epoch 16\n",
      "-------------------\n",
      "loss: 0.232426 batch:     0\n",
      "loss: 0.322771 batch:     8\n",
      "loss: 0.260361 batch:    16\n",
      "Epoch 17\n",
      "-------------------\n",
      "loss: 0.167751 batch:     0\n",
      "loss: 0.184633 batch:     8\n",
      "loss: 0.190588 batch:    16\n",
      "Epoch 18\n",
      "-------------------\n",
      "loss: 0.167175 batch:     0\n",
      "loss: 0.132611 batch:     8\n",
      "loss: 0.224921 batch:    16\n",
      "Epoch 19\n",
      "-------------------\n",
      "loss: 0.147332 batch:     0\n",
      "loss: 0.167616 batch:     8\n",
      "loss: 0.117841 batch:    16\n",
      "Epoch 20\n",
      "-------------------\n",
      "loss: 0.100910 batch:     0\n",
      "loss: 0.084571 batch:     8\n",
      "loss: 0.132041 batch:    16\n",
      "Epoch 21\n",
      "-------------------\n",
      "loss: 0.087616 batch:     0\n",
      "loss: 0.084723 batch:     8\n",
      "loss: 0.106668 batch:    16\n",
      "Epoch 22\n",
      "-------------------\n",
      "loss: 0.104257 batch:     0\n",
      "loss: 0.074940 batch:     8\n",
      "loss: 0.083603 batch:    16\n",
      "Epoch 23\n",
      "-------------------\n",
      "loss: 0.067530 batch:     0\n",
      "loss: 0.065115 batch:     8\n",
      "loss: 0.053738 batch:    16\n",
      "Epoch 24\n",
      "-------------------\n",
      "loss: 0.059225 batch:     0\n",
      "loss: 0.059888 batch:     8\n",
      "loss: 0.047620 batch:    16\n",
      "Epoch 25\n",
      "-------------------\n",
      "loss: 0.063949 batch:     0\n",
      "loss: 0.051399 batch:     8\n",
      "loss: 0.062057 batch:    16\n",
      "Epoch 26\n",
      "-------------------\n",
      "loss: 0.052068 batch:     0\n",
      "loss: 0.048624 batch:     8\n",
      "loss: 0.052953 batch:    16\n",
      "Epoch 27\n",
      "-------------------\n",
      "loss: 0.042074 batch:     0\n",
      "loss: 0.029552 batch:     8\n",
      "loss: 0.040587 batch:    16\n",
      "Epoch 28\n",
      "-------------------\n",
      "loss: 0.037615 batch:     0\n",
      "loss: 0.033411 batch:     8\n",
      "loss: 0.039424 batch:    16\n",
      "Epoch 29\n",
      "-------------------\n",
      "loss: 0.036780 batch:     0\n",
      "loss: 0.036985 batch:     8\n",
      "loss: 0.037534 batch:    16\n",
      "Epoch 30\n",
      "-------------------\n",
      "loss: 0.031989 batch:     0\n",
      "loss: 0.031035 batch:     8\n",
      "loss: 0.030474 batch:    16\n",
      "Epoch 31\n",
      "-------------------\n",
      "loss: 0.030533 batch:     0\n",
      "loss: 0.025334 batch:     8\n",
      "loss: 0.024742 batch:    16\n",
      "Epoch 32\n",
      "-------------------\n",
      "loss: 0.026719 batch:     0\n",
      "loss: 0.019267 batch:     8\n",
      "loss: 0.027043 batch:    16\n",
      "Epoch 33\n",
      "-------------------\n",
      "loss: 0.037577 batch:     0\n",
      "loss: 0.030282 batch:     8\n",
      "loss: 0.025184 batch:    16\n",
      "Epoch 34\n",
      "-------------------\n",
      "loss: 0.029876 batch:     0\n",
      "loss: 0.027669 batch:     8\n",
      "loss: 0.027434 batch:    16\n",
      "Epoch 35\n",
      "-------------------\n",
      "loss: 0.026860 batch:     0\n",
      "loss: 0.018887 batch:     8\n",
      "loss: 0.020201 batch:    16\n",
      "Epoch 36\n",
      "-------------------\n",
      "loss: 0.019334 batch:     0\n",
      "loss: 0.018062 batch:     8\n",
      "loss: 0.015991 batch:    16\n",
      "Epoch 37\n",
      "-------------------\n",
      "loss: 0.020779 batch:     0\n",
      "loss: 0.017033 batch:     8\n",
      "loss: 0.021934 batch:    16\n",
      "Epoch 38\n",
      "-------------------\n",
      "loss: 0.016825 batch:     0\n",
      "loss: 0.016004 batch:     8\n",
      "loss: 0.017847 batch:    16\n",
      "Epoch 39\n",
      "-------------------\n",
      "loss: 0.016921 batch:     0\n",
      "loss: 0.016048 batch:     8\n",
      "loss: 0.017511 batch:    16\n",
      "Epoch 40\n",
      "-------------------\n",
      "loss: 0.017407 batch:     0\n",
      "loss: 0.013181 batch:     8\n",
      "loss: 0.020849 batch:    16\n",
      "Epoch 41\n",
      "-------------------\n",
      "loss: 0.018521 batch:     0\n",
      "loss: 0.014954 batch:     8\n",
      "loss: 0.015423 batch:    16\n",
      "Epoch 42\n",
      "-------------------\n",
      "loss: 0.012836 batch:     0\n",
      "loss: 0.015052 batch:     8\n",
      "loss: 0.014850 batch:    16\n",
      "Epoch 43\n",
      "-------------------\n",
      "loss: 0.013830 batch:     0\n",
      "loss: 0.010661 batch:     8\n",
      "loss: 0.014762 batch:    16\n",
      "Epoch 44\n",
      "-------------------\n",
      "loss: 0.010908 batch:     0\n",
      "loss: 0.013125 batch:     8\n",
      "loss: 0.013278 batch:    16\n",
      "Epoch 45\n",
      "-------------------\n",
      "loss: 0.011496 batch:     0\n",
      "loss: 0.013268 batch:     8\n",
      "loss: 0.015883 batch:    16\n",
      "Epoch 46\n",
      "-------------------\n",
      "loss: 0.016677 batch:     0\n",
      "loss: 0.011349 batch:     8\n",
      "loss: 0.009279 batch:    16\n",
      "Epoch 47\n",
      "-------------------\n",
      "loss: 0.008019 batch:     0\n",
      "loss: 0.011033 batch:     8\n",
      "loss: 0.014718 batch:    16\n",
      "Epoch 48\n",
      "-------------------\n",
      "loss: 0.010344 batch:     0\n",
      "loss: 0.010232 batch:     8\n",
      "loss: 0.008782 batch:    16\n",
      "Epoch 49\n",
      "-------------------\n",
      "loss: 0.012046 batch:     0\n",
      "loss: 0.009483 batch:     8\n",
      "loss: 0.009836 batch:    16\n",
      "Epoch 50\n",
      "-------------------\n",
      "loss: 0.009661 batch:     0\n",
      "loss: 0.007921 batch:     8\n",
      "loss: 0.014225 batch:    16\n",
      "Epoch 51\n",
      "-------------------\n",
      "loss: 0.007774 batch:     0\n",
      "loss: 0.009484 batch:     8\n",
      "loss: 0.008062 batch:    16\n",
      "Epoch 52\n",
      "-------------------\n",
      "loss: 0.008803 batch:     0\n",
      "loss: 0.010118 batch:     8\n",
      "loss: 0.009104 batch:    16\n",
      "Epoch 53\n",
      "-------------------\n",
      "loss: 0.009310 batch:     0\n",
      "loss: 0.007996 batch:     8\n",
      "loss: 0.009284 batch:    16\n",
      "Epoch 54\n",
      "-------------------\n",
      "loss: 0.007609 batch:     0\n",
      "loss: 0.007203 batch:     8\n",
      "loss: 0.009697 batch:    16\n",
      "Epoch 55\n",
      "-------------------\n",
      "loss: 0.009271 batch:     0\n",
      "loss: 0.008134 batch:     8\n",
      "loss: 0.007444 batch:    16\n",
      "Epoch 56\n",
      "-------------------\n",
      "loss: 0.006202 batch:     0\n",
      "loss: 0.009561 batch:     8\n",
      "loss: 0.006521 batch:    16\n",
      "Epoch 57\n",
      "-------------------\n",
      "loss: 0.006302 batch:     0\n",
      "loss: 0.006499 batch:     8\n",
      "loss: 0.006653 batch:    16\n",
      "Epoch 58\n",
      "-------------------\n",
      "loss: 0.005351 batch:     0\n",
      "loss: 0.006616 batch:     8\n",
      "loss: 0.006558 batch:    16\n",
      "Epoch 59\n",
      "-------------------\n",
      "loss: 0.006505 batch:     0\n",
      "loss: 0.005988 batch:     8\n",
      "loss: 0.005946 batch:    16\n",
      "Epoch 60\n",
      "-------------------\n",
      "loss: 0.006074 batch:     0\n",
      "loss: 0.007054 batch:     8\n",
      "loss: 0.006026 batch:    16\n",
      "Epoch 61\n",
      "-------------------\n",
      "loss: 0.005323 batch:     0\n",
      "loss: 0.005019 batch:     8\n",
      "loss: 0.008489 batch:    16\n",
      "Epoch 62\n",
      "-------------------\n",
      "loss: 0.005426 batch:     0\n",
      "loss: 0.005827 batch:     8\n",
      "loss: 0.006616 batch:    16\n",
      "Epoch 63\n",
      "-------------------\n",
      "loss: 0.005058 batch:     0\n",
      "loss: 0.006260 batch:     8\n",
      "loss: 0.007075 batch:    16\n",
      "Epoch 64\n",
      "-------------------\n",
      "loss: 0.004964 batch:     0\n",
      "loss: 0.006422 batch:     8\n",
      "loss: 0.004692 batch:    16\n",
      "Epoch 65\n",
      "-------------------\n",
      "loss: 0.004861 batch:     0\n",
      "loss: 0.006148 batch:     8\n",
      "loss: 0.004886 batch:    16\n",
      "Epoch 66\n",
      "-------------------\n",
      "loss: 0.004958 batch:     0\n",
      "loss: 0.004853 batch:     8\n",
      "loss: 0.004249 batch:    16\n",
      "Epoch 67\n",
      "-------------------\n",
      "loss: 0.004011 batch:     0\n",
      "loss: 0.007489 batch:     8\n",
      "loss: 0.005191 batch:    16\n",
      "Epoch 68\n",
      "-------------------\n",
      "loss: 0.004379 batch:     0\n",
      "loss: 0.007769 batch:     8\n",
      "loss: 0.005013 batch:    16\n",
      "Epoch 69\n",
      "-------------------\n",
      "loss: 0.003893 batch:     0\n",
      "loss: 0.005097 batch:     8\n",
      "loss: 0.003564 batch:    16\n",
      "Epoch 70\n",
      "-------------------\n",
      "loss: 0.004318 batch:     0\n",
      "loss: 0.004461 batch:     8\n",
      "loss: 0.004546 batch:    16\n",
      "Epoch 71\n",
      "-------------------\n",
      "loss: 0.005062 batch:     0\n",
      "loss: 0.003919 batch:     8\n",
      "loss: 0.004224 batch:    16\n",
      "Epoch 72\n",
      "-------------------\n",
      "loss: 0.003801 batch:     0\n",
      "loss: 0.004179 batch:     8\n",
      "loss: 0.004810 batch:    16\n",
      "Epoch 73\n",
      "-------------------\n",
      "loss: 0.004228 batch:     0\n",
      "loss: 0.004420 batch:     8\n",
      "loss: 0.004001 batch:    16\n",
      "Epoch 74\n",
      "-------------------\n",
      "loss: 0.004601 batch:     0\n",
      "loss: 0.004011 batch:     8\n",
      "loss: 0.004412 batch:    16\n",
      "Epoch 75\n",
      "-------------------\n",
      "loss: 0.003524 batch:     0\n",
      "loss: 0.003348 batch:     8\n",
      "loss: 0.003688 batch:    16\n",
      "Epoch 76\n",
      "-------------------\n",
      "loss: 0.003551 batch:     0\n",
      "loss: 0.004141 batch:     8\n",
      "loss: 0.003494 batch:    16\n",
      "Epoch 77\n",
      "-------------------\n",
      "loss: 0.003968 batch:     0\n",
      "loss: 0.003587 batch:     8\n",
      "loss: 0.003309 batch:    16\n",
      "Epoch 78\n",
      "-------------------\n",
      "loss: 0.003397 batch:     0\n",
      "loss: 0.003193 batch:     8\n",
      "loss: 0.003978 batch:    16\n",
      "Epoch 79\n",
      "-------------------\n",
      "loss: 0.002920 batch:     0\n",
      "loss: 0.003636 batch:     8\n",
      "loss: 0.003210 batch:    16\n",
      "Epoch 80\n",
      "-------------------\n",
      "loss: 0.003317 batch:     0\n",
      "loss: 0.003799 batch:     8\n",
      "loss: 0.003501 batch:    16\n",
      "Epoch 81\n",
      "-------------------\n",
      "loss: 0.003318 batch:     0\n",
      "loss: 0.002879 batch:     8\n",
      "loss: 0.002996 batch:    16\n",
      "Epoch 82\n",
      "-------------------\n",
      "loss: 0.002336 batch:     0\n",
      "loss: 0.002551 batch:     8\n",
      "loss: 0.003253 batch:    16\n",
      "Epoch 83\n",
      "-------------------\n",
      "loss: 0.002721 batch:     0\n",
      "loss: 0.003111 batch:     8\n",
      "loss: 0.003260 batch:    16\n",
      "Epoch 84\n",
      "-------------------\n",
      "loss: 0.003266 batch:     0\n",
      "loss: 0.002537 batch:     8\n",
      "loss: 0.003152 batch:    16\n",
      "Epoch 85\n",
      "-------------------\n",
      "loss: 0.003949 batch:     0\n",
      "loss: 0.003029 batch:     8\n",
      "loss: 0.003316 batch:    16\n",
      "Epoch 86\n",
      "-------------------\n",
      "loss: 0.002912 batch:     0\n",
      "loss: 0.002498 batch:     8\n",
      "loss: 0.004364 batch:    16\n",
      "Epoch 87\n",
      "-------------------\n",
      "loss: 0.002807 batch:     0\n",
      "loss: 0.002835 batch:     8\n",
      "loss: 0.003248 batch:    16\n",
      "Epoch 88\n",
      "-------------------\n",
      "loss: 0.002338 batch:     0\n",
      "loss: 0.003170 batch:     8\n",
      "loss: 0.002860 batch:    16\n",
      "Epoch 89\n",
      "-------------------\n",
      "loss: 0.002671 batch:     0\n",
      "loss: 0.002489 batch:     8\n",
      "loss: 0.002817 batch:    16\n",
      "Epoch 90\n",
      "-------------------\n",
      "loss: 0.002316 batch:     0\n",
      "loss: 0.002418 batch:     8\n",
      "loss: 0.002455 batch:    16\n",
      "Epoch 91\n",
      "-------------------\n",
      "loss: 0.002316 batch:     0\n",
      "loss: 0.003602 batch:     8\n",
      "loss: 0.002560 batch:    16\n",
      "Epoch 92\n",
      "-------------------\n",
      "loss: 0.002227 batch:     0\n",
      "loss: 0.002191 batch:     8\n",
      "loss: 0.002075 batch:    16\n",
      "Epoch 93\n",
      "-------------------\n",
      "loss: 0.002443 batch:     0\n",
      "loss: 0.002645 batch:     8\n",
      "loss: 0.001898 batch:    16\n",
      "Epoch 94\n",
      "-------------------\n",
      "loss: 0.002135 batch:     0\n",
      "loss: 0.002504 batch:     8\n",
      "loss: 0.002706 batch:    16\n",
      "Epoch 95\n",
      "-------------------\n",
      "loss: 0.002323 batch:     0\n",
      "loss: 0.002172 batch:     8\n",
      "loss: 0.002303 batch:    16\n",
      "Epoch 96\n",
      "-------------------\n",
      "loss: 0.002322 batch:     0\n",
      "loss: 0.002816 batch:     8\n",
      "loss: 0.002009 batch:    16\n",
      "Epoch 97\n",
      "-------------------\n",
      "loss: 0.002312 batch:     0\n",
      "loss: 0.002120 batch:     8\n",
      "loss: 0.002498 batch:    16\n",
      "Epoch 98\n",
      "-------------------\n",
      "loss: 0.002174 batch:     0\n",
      "loss: 0.001689 batch:     8\n",
      "loss: 0.002494 batch:    16\n",
      "Epoch 99\n",
      "-------------------\n",
      "loss: 0.001997 batch:     0\n",
      "loss: 0.002326 batch:     8\n",
      "loss: 0.001866 batch:    16\n",
      "Epoch 100\n",
      "-------------------\n",
      "loss: 0.001890 batch:     0\n",
      "loss: 0.001650 batch:     8\n",
      "loss: 0.002554 batch:    16\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "lr = 1e-3\n",
    "\n",
    "model = OneLayerNet()\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------\")\n",
    "    train_model.train_loop(dataloader, model, loss_fn, optimizer, print_every=8)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"cifar10_classifier/results/one_layer_net\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
